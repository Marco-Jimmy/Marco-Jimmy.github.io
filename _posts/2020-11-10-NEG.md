---
layout:     post
title:      Negative sampling in NLP
subtitle:   Distributed Representations of Words and Phrases and their Compositionality
date:       2020-10-24
author:     JIM
header-img: img/post-bg-desk.jpg
mathjax: true
catalog: true
tags:
    - Foreseer
    - ML
    - Paper
    - Notes
    - NLP
    - Negative Sampling
    - Language Model
  
---

# Negative Sampling in NLP
This post discussed the language model in the Natural Language Processing (NLP) based on skip-gram model. To be more specfic, I focus on the mechanism of **negative sampling**.
## Background knowledge

The task of **language model** is to predict a word given a context. Usually, the next word of some imcompleted sentence is to be predicted. For example,

> We are going to train the neural ______

The answer should be something like network, right?
However, in this post, the skip-gram model is used, which is used to predict the surrounding words given a center word. 

### Word Vectors & Skip-gram Model

**Word vectors** are important tools to solve this problem, and are fundamental tools to help solve many NLP-related problem. The basic method is: each word in the volcabulary is assigned a vector of dimsion $n$ so that the vectors can reflect the relationship between words. For example, boy and man will be close to each other in the embedded space. But how do we figure out the value of these word vectors? A way is to use the **Skip-gram model**.


The **skip-gram model** gives an efficient method for learning high-quality distributed vector representations that can capture a large number of precise syntactic and semantic word relationships. The training objective of the Skip-gram model is to find word representation that are useful for predicting the surrounding words in a sentence or a document. More formally, given a sequence of trainging words $w_1,w_2,...,w_T$, where $T$ is the size of our corpus, the objective of the skip-gram model is to maximize the **average log probability**
$$
\begin{aligned}
\dfrac{1}{T}\sum^T_{t=1}\sum_{-c\leq j\leq c,j\neq 0}\log p(w_{t+j}|w_t)    
\end{aligned}
$$
where $c$ is the size of the training context (controlled by user).

The function $p$ is defined as below:
$$
p(w_O|w_I)=\dfrac{\exp({v'_{w_O}}^T v_{w_I})}{\sum_{w=1}^W\exp({v'_{w}}^T v_{w_I})}
$$
Notice that $v$ is the "input" vector - the center word while $v'$ is the "output" vector - the predicted word. $W$ is the number of words in the vocabulary.

We then optimize the word vector representation so that the average log probability is maximized, and the word vectors are just the hidden layer of the neural network. This method is called *fake task*.

## Negative Sampling
There are several downsides of the original skip-gram model:

1. Calculating $p$'s denominator is expensive in time complexity, which is $O(W)$. It seems to be the major problem.
2. Each time a data sample being fed to the network, the whole network will be updated slightly, while the network is large in size.
3. We don't need to tell the neural network that all $W-1$ words are not the target all the time, we just need to tell the neural network these aren't the target words some of the time.
   
The author proposed a method called **negative sampling** to solve this problem. The negative sampling (NEG) is defined by 
$$
\log p(w_O|w_I)= \log\sigma({v'_{w_O}}^T v_{w_I})+\sum_{i=1}^k \mathbb{E}_{w_i\sim P_n(w)}\left[\log \sigma(-{v'_{w_i}}^T v_{w_I}) \right]
$$
where 
* $\sigma(\cdot)$ is the sigmoid function $\dfrac{1}{1+\exp(\cdot)}$
* $P_n(w)$ is the noise distribution. The author found that the unigram distribution $U(w)$ raised to $3/4$rd power outperformed significantly the unigram and the uniform distribution. 
* $k$ is the number of negative samples for each data sample. The author argued that 5-20 is suitable for small training datasets and 2-5 is suitable for larger ones.