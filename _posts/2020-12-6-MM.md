---
layout: post
title: MM algorithm
subtitle: Majorize-Minimization & Minorize-Maximization
date: 2020-12-6
author: JIM
header-img: img/post-bg-coffee.jpeg
catalog: true
mathjax: true
tags:
  - Algorithm
  - Convex Optimization
---

The **MM algorithm** is an iterative optimization method which exploits the convexity of a function in order to find its maxima or minima. The MM stands for **Majorize-Minimization** or **Minorize-Maximization**, depending on whether the desired optimization is a maximization or a minimization. MM itself is not an algorithm, but a description of how to construct an optimization algorithm.

## Algorithm

The MM algorithm works by finding a surrogate function that minorizes or majorizes the objective function. Optimizing the surrogate function will either improve the value of the objective function or leave it unchanged.

Taking the minorize-maximization version, let $f(\theta)$ be the objective concave function to be maximized. At the $m$ step of the algorithm, $m=0,1 \ldots,$ the constructed function $g\left(\theta \mid \theta^{(m)}\right)$ will be called the minorized version of the objective function (the surrogate function) at $\theta^{(m)}$ if

$$

g\left(\theta \mid \theta^{(m)}\right) \leq f(\theta) \text { for all } \theta
$$

$$
g\left(\theta^{(m)} \mid \theta^{(m)}\right)=f\left(\theta^{(m)}\right)
$$

Then, maximize $g\left(\theta \mid \theta^{(m)}\right)$ instead of $f(\theta),$ and let

$$
\theta^{(m+1)}=\arg \max _{\theta} g\left(\theta \mid \theta^{(m)}\right)
$$

The above iterative method will guarantee that $f\left(\theta^{(m)}\right)$ will converge to a local optimum or a saddle point as $m$ goes to infinity. By the above construction $f\left(\theta^{(m+1)}\right) \geq g\left(\theta^{(m+1)} \mid \theta^{(m)}\right) \geq g\left(\theta^{(m)} \mid \theta^{(m)}\right)=f\left(\theta^{(m)}\right)$

![mm algorithm](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Mmalgorithm.jpg/330px-Mmalgorithm.jpg)